{"cells":[{"cell_type":"code","source":["import quandl\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nimport pyspark\nfrom pyspark.sql import functions as sqlf\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.lines as mlines\nimport numpy as np\nimport time\nimport pandas as pd\nimport StringIO\nimport random\nfrom random import randint\nfrom time import gmtime, strftime\nimport os\nimport re\nimport json\nimport matha\nimport pytz\nimport datetime\n\n# Access config\n# The values were randomized, update with your own Amazon S3 details.\nACCESS_KEY = \"AKIZIGCCH4PVSGRZD7MQ\"\nSECRET_KEY = \"LPv8QAFxmiSJNKJRNqlMzzhml1X0pXBCDOF1dpm6\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"bucket-name\"\nLOGFILE = 'log'\nTRIAL_LOGFILE = 'trial_log.parquet'\nOPTIMIZED_TRIAL_LOGFILE = 'optimized_trial_log.parquet'\nTEST_LOGFILE = 'test_log.parquet'\nEXP_RESULTS = 'exp_results.json'\n\nQ_FILE = 'Q.csv'\nCONFIG_FILE = 'config.json'\n\n# Unmount and mount disk. If there is an error, ucomment the following unmount code line.\ndbutils.fs.unmount(\"/mnt\")\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt\")\n\n# See if csv files in details directory exist.\nPROJECT_DIR = \"/mnt/rl_trading\"\ndisplay(dbutils.fs.ls(PROJECT_DIR))\n\n# Quandl setup - Update with your own Quandle API key.\nquandl.ApiConfig.api_key = 'zhiR5Rz7eJqp_XNcZz3c'"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Helper functions\ndef discretize_bins(data, steps=10):\n  \"\"\" Discretization thresholds.\n  \"\"\"\n  npdata = np.array(data)\n  if npdata.shape[0] < steps:\n    raise ValueError(\n      'number of discretization steps (currently {}) must be equal or higher than number of data (currently {})'.format(steps, npdata.shape[0]))\n  stepsize = npdata.shape[0] / steps\n  npdata = np.sort(npdata)\n  th = np.arange(steps)\n  th = npdata[((th+1)*stepsize)-1]\n  return np.array(th)\n\ndef discretize(value, thresholds):\n  \"\"\" Convert real value to integer index.\n  \"\"\"\n  if type(thresholds) == list:\n    thresholds = np.array(thresholds)\n  idx_r = np.where(thresholds >= value)[0]\n  if len(idx_r) == 0:\n    if value < thresholds[0]:\n      idx = 0\n    elif value > thresholds[-1]:\n      idx = thresholds.shape[0]-1      \n  else:\n    idx = idx_r[0]\n  return idx\n\nth = discretize_bins(np.array(\n    [0.1, 0.2, 0.3, 0.31, 4, 5, 6, 20, 21, 22]), steps=3)\nassert(discretize(0.15, th) == 0)\nassert(discretize(5, th) == 1)\nassert(discretize(20.5, th) == 2)\n  \ndef combine_ints(intlist, digits=None):\n  \"\"\" Combine multiple integers\n  \"\"\"\n  if digits is None:\n    digits = len(str(max(intlist)))\n  combined = ''\n  for i in intlist:\n    combined += str(i).zfill(digits)\n  return int(combined)\n\nassert(combine_ints([10, 53, 10, 7]) == 10531007)\nassert(combine_ints([0, 5, 10, 7]) == 51007)\nassert(combine_ints([6, 0], digits=2) == 600)\n\ndef split_ints(combined, digits, size=None):\n  \"\"\" Split combined integers\n  \n  Args:\n  - digits(int): Number of digits, which can be found by counting the digits of\n                 `discretization_steps` of a States object.\n  - size(int): Size of final intlist. Can be found by counting\n               number of columns of a States object. Inferred from digits when None.\n  \"\"\"\n  combined = str(combined)\n  if size is None:\n    size = len(combined) / digits\n  combined = combined.zfill(size*digits)\n  intlist = []\n  for i in range(digits, len(combined)+1, digits):\n    pos1 = len(combined)-i\n    pos2 = pos1 + digits\n    intlist.insert(0, int(combined[pos1:pos2]))\n  return intlist\nassert(split_ints(10531007, digits=2) == [10, 53, 10, 7])\nassert(split_ints(51007, digits=2, size=4) == [0, 5, 10, 7])\nassert(split_ints(600, digits=2, size=2) == [6, 0])\n    \ndef to_epoch(date_time, pattern='%Y-%m-%d %H:%M:%S'):\n  \"\"\" Convert Pandas Timeframe object into epoch.\"\"\"\n  epoch = int(time.mktime(time.strptime(str(date_time), pattern)))\n  return epoch\n\ndef to_datetime(epoch):\n  \"\"\" Convert epoch into Pandas Timeframe object.\"\"\"\n  return pd.to_datetime(epoch, unit='s')\n\ndef str2t(text, pattern='%Y-%m-%d'):\n  \"\"\" Convert date string to Timestamp object.\"\"\"\n  return pd.DatetimeIndex([\n  datetime.datetime.strptime(\n    text, pattern)])[0]\n\ndef create_Q_from_file(Q_file):\n  s = StringIO.StringIO()\n  s.write(dbutils.fs.head(Q_file))\n  s.seek(0)\n  df = pd.read_csv(s)\n  Q = {}\n  for equity in frozenset(df['equity']):\n    Q[equity] = df.loc[df['equity'] == equity, df.columns[2:]] \\\n      .set_index('state').transpose().to_dict()\n  return Q\n\ndef get_rolling_mean(values, window):\n  \"\"\" Return rolling mean of given values, using specified window size.\n  \"\"\"\n  return pd.rolling_mean(values, window=window)\n\n\ndef get_rolling_std(values, window):\n  \"\"\" Return rolling standard deviation of given values, using specified window size.\n  \"\"\"\n  return pd.rolling_std(values, window=window)\n\n\ndef get_bollinger_bands(rm, rstd):\n  \"\"\" Return upper and lower Bollinger Bands. \"\"\"\n  upper_band = rm + 2 * rstd\n  lower_band = rm - 2 * rstd\n  return upper_band, lower_band\n# END - Helper functions"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Report-related functions\ndef make_tz_aware(df):\n  awaretime = []\n  for idx in df.index:\n    awaretime.append(idx.replace(tzinfo=pytz.UTC))\n  df = df.reindex(awaretime)\n  return df\n  \ndef make_test_df_tz_aware(test_df):\n  diff_df = test_df.copy()\n  diff_df = diff_df.set_index('time')\n  diff_df = make_tz_aware(diff_df)\n  return diff_df\n\ndef select_test_df_treasury_duration(test_df):\n  first_idx = test_df.index[0]\n  last_idx = max(test_df.index)\n  start_date = test_df['time'][first_idx]\n  end_date = test_df['time'][last_idx]\n  return select_treasury_duration(start_date, end_date)\n  \ndef calculate_beta(test_df, bm, tdf):\n  dur = select_test_df_treasury_duration(test_df)\n  rfr = tdf[dur]\n  df = make_test_df_tz_aware(test_df)\n  df = df['return_pct'] - rfr\n  bm = make_tz_aware(bm)\n  bm = bm - rfr\n  return df.cov(bm)/bm.var()\n\ndef calculate_sharpe(test_df, tdf, col='return_pct'):\n  \"\"\"\n  Args:\n    test_df(DataFrame): A pandas dataframe of the data. Must have column 'time' and 'return_pct'.\n    tdf(DataFrame): A pandas dataframe returned by function `get_treasury_data`.\n  \"\"\"\n  if col == 'return_pct':\n    dur = select_test_df_treasury_duration(test_df)\n    rfr = tdf[dur] # daily risk-free-return\n    diff_df = make_test_df_tz_aware(test_df)\n    diff_df = diff_df['return_pct'] - rfr\n  else:\n    start_date = min(test_df.index)\n    end_date = max(test_df.index)\n    dur = select_treasury_duration(start_date, end_date)\n    rfr = tdf[dur] # daily risk-free-return\n    diff_df = make_tz_aware(test_df)\n    if col is None:\n      diff_df = diff_df - rfr\n    else:\n      diff_df = diff_df[col] - rfr\n  return math.sqrt(252) * diff_df.mean()/diff_df.std(ddof=1)\n\ndef max_dd(ser):\n  \"\"\" Max Dropdown \"\"\"\n  max2here = pd.expanding_max(ser)\n  dd2here = ser - max2here\n  return dd2here.min()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# https://github.com/quantopian/zipline/blob/master/zipline/data/treasuries.py\n#\n# Copyright 2013 Quantopian, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.  \nfrom operator import itemgetter\nimport re\n\nimport numpy as np\nimport pandas as pd\n\n\nget_unit_and_periods = itemgetter('unit', 'periods')\n\n\ndef parse_treasury_csv_column(column):\n    \"\"\"\n    Parse a treasury CSV column into a more human-readable format.\n\n    Columns start with 'RIFLGFC', followed by Y or M (year or month), followed\n    by a two-digit number signifying number of years/months, followed by _N.B.\n    We only care about the middle two entries, which we turn into a string like\n    3month or 30year.\n    \"\"\"\n    column_re = re.compile(\n        r\"^(?P<prefix>RIFLGFC)\"\n        \"(?P<unit>[YM])\"\n        \"(?P<periods>[0-9]{2})\"\n        \"(?P<suffix>_N.B)$\"\n    )\n\n    match = column_re.match(column)\n    if match is None:\n        raise ValueError(\"Couldn't parse CSV column %r.\" % column)\n    unit, periods = get_unit_and_periods(match.groupdict())\n\n    # Roundtrip through int to coerce '06' into '6'.\n    return str(int(periods)) + ('year' if unit == 'Y' else 'month')\n\n\ndef earliest_possible_date():\n    \"\"\"\n    The earliest date for which we can load data from this module.\n    \"\"\"\n    # The US Treasury actually has data going back further than this, but it's\n    # pretty rare to find pricing data going back that far, and there's no\n    # reason to make people download benchmarks back to 1950 that they'll never\n    # be able to use.\n    return pd.Timestamp('1980', tz='UTC')\n\n\ndef get_treasury_data(start_date, end_date):\n    return pd.read_csv(\n        \"http://www.federalreserve.gov/datadownload/Output.aspx\"\n        \"?rel=H15\"\n        \"&series=bf17364827e38702b42a58cf8eaa3f78\"\n        \"&lastObs=\"\n        \"&from=\"  # An unbounded query is ~2x faster than specifying dates.\n        \"&to=\"\n        \"&filetype=csv\"\n        \"&label=omit\"\n        \"&layout=seriescolumn\"\n        \"&type=package\",\n        skiprows=1,  # First row is a useless header.\n        parse_dates=['Time Period'],\n        na_values=['ND'],  # Presumably this stands for \"No Data\".\n        index_col=0,\n    ).loc[\n        start_date:end_date\n    ].dropna(\n        how='all'\n    ).rename(\n        columns=parse_treasury_csv_column\n    ).tz_localize('UTC') * 0.01  # Convert from 2.57% to 0.0257.\n\n\ndef dataconverter(s):\n    try:\n        return float(s) / 100\n    except:\n        return np.nan\n\n\ndef get_daily_10yr_treasury_data():\n    \"\"\"Download daily 10 year treasury rates from the Federal Reserve and\n    return a pandas.Series.\"\"\"\n    url = \"http://www.federalreserve.gov/datadownload/Output.aspx?rel=H15\" \\\n          \"&series=bcb44e57fb57efbe90002369321bfb3f&lastObs=&from=&to=\" \\\n          \"&filetype=csv&label=include&layout=seriescolumn\"\n    return pd.read_csv(url, header=5, index_col=0, names=['DATE', 'BC_10YEAR'],\n                       parse_dates=True, converters={1: dataconverter},\n                       squeeze=True)\n  \ndef select_treasury_duration(start_date, end_date):  \n    td = end_date - start_date  \n    if td.days <= 31:  \n        treasury_duration = '1month'  \n    elif td.days <= 93:  \n        treasury_duration = '3month'  \n    elif td.days <= 186:  \n        treasury_duration = '6month'  \n    elif td.days <= 366:  \n        treasury_duration = '1year'  \n    elif td.days <= 365 * 2 + 1:  \n        treasury_duration = '2year'  \n    elif td.days <= 365 * 3 + 1:  \n        treasury_duration = '3year'  \n    elif td.days <= 365 * 5 + 2:  \n        treasury_duration = '5year'  \n    elif td.days <= 365 * 7 + 2:  \n        treasury_duration = '7year'  \n    elif td.days <= 365 * 10 + 2:  \n        treasury_duration = '10year'  \n    else:  \n        treasury_duration = '30year'\n\n    return treasury_duration"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["class Commission(object):\n  \"\"\" Commission Model \"\"\"\n  def __init__(self, per_share=0.0075, min_cost=1):\n    self.per_share = per_share\n    self.min_cost = min_cost\n  def calculate(self, num_share):\n    c = num_share * self.per_share\n    if c < self.min_cost:\n      c = self.min_cost\n    return c"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["class Environment(object):\n  \"\"\" Environment of this project.\n  \n  About equity_events property:\n  Each equity has one huge events data stored via DataFrame.\n    \n  This is a bit different from experience tuple stored in the\n  agent that does not require dates:\n  (state, action, new_state, reward)\n  \"\"\"\n  def __init__(self, capital=100000, logger=None,\n               commission=None,\n               discretization_steps=20, config=None):\n    self.valid_actions = [None, 'buy', 'sell']\n    self.commission = commission\n\n    # All traders in the environment. Each trader\n    # tracks a single equity.\n    self.trader_states = {}\n\n    # DataFrames\n    self.equity_events = {}\n        \n    # List of thresholds returned by `discretize_bins` method.\n    # Each equity has one list.\n    # This will be used in discretization steps.\n    self.equity_bins = {}\n    \n    self.trader = None\n    self.done = False\n    self.Q_to_load = None\n    \n    self.logger = logger\n    self.capital = capital\n    self.discretization_steps = discretization_steps\n    \n    self.resetted = False\n\n    if config is not None:\n      for key in config.keys():\n        setattr(self, key, config[key])\n    \n  def _join_dfs(self, df1, df2,\n                 exclude_cols='time', on=None, how='inner'):\n    if type(exclude_cols) == str:\n      exclude_cols = [exclude_cols]\n      \n    if not on:\n      on = (df1[exclude_cols[0]] == df2[exclude_cols[0]])\n\n    return df1.join(df2, on=on, how=how) \\\n      .select(map(lambda x: df1[x], df1.columns) + \n        map(lambda x: df2[x],\n          [cols for cols in df2.columns if cols not in exclude_cols]))\n  \n  def log(self, text):\n    if self.logger is None:\n      print text\n    else:\n      self.logger.write(text)\n  \n  def log_trial(self, *args, **kwargs):\n    if self.logger is None:\n      print var_dict\n    else:\n      self.logger.log_trial(*args, **kwargs)\n\n  def load_Q(self, Q_file):\n    # Todo: Load csv\n    self.Q_to_load = sc.parallelize()\n    if self.trader != None:\n      self.trader.load_Q(self.Q_to_load)\n    return self\n  \n  def save_Q(self, Q_file):\n    if self.trader != None:\n      rows = []\n      fields = ['equity', 'state', 'hold', 'buy', 'sell']\n      for state, actions in self.trader.Q.iteritems():\n        row = [self.trader.equity,\n               state,\n               actions['hold'], \n               actions['buy'],\n               actions['sell']\n              ]\n        rows.append(row)\n      df = pd.DataFrame(rows, columns=fields)\n      s = StringIO.StringIO()\n      df.to_csv(s)\n      dbutils.fs.put(Q_file, s.getvalue(), overwrite=True)\n      \n  def prepare_discretization_bins(self):\n    \"\"\" Prepare the bins/thresholds for discretization step.\n    \n    This is an expensive function, as it loops over each column of\n    each event_df, get all data, and extract the bins using\n    `discretize_bins` helper method.\n    \"\"\"\n    for equity, event_df in self.equity_events.iteritems():\n      for column in event_df.columns:\n        data = event_df.select(column).rdd.flatMap(list).collect()\n        if equity not in self.equity_bins:\n          self.equity_bins[equity] = {}\n        self.log(\"create bins from this data:\\n{}\\nsteps:{}\".format(\n            data, self.discretization_steps))\n        self.equity_bins[equity][column] = discretize_bins(\n          data,\n          self.discretization_steps).tolist()\n        self.log(\"created bins: {}\".format(\n            self.equity_bins[equity][column]))\n        \n  def current_event(self, events_df):\n    \"\"\" A shortcut to get the current event.\"\"\"\n    if self.current_datetime_ is None:\n      event = events_df.sort(sqlf.asc('time')).first()\n    else:\n      event = events_df.filter(\n          events_df['time'] == self.current_datetime_\n      ).first()\n    return event\n    \n  def next_event(self, events_df):\n    \"\"\" A shortcut to get the next event (usually next day).\"\"\"\n    if self.current_datetime_ is None:\n      event = self.current_event(events_df)\n    else:\n      event = events_df.filter(\n          events_df['time'] > self.current_datetime_\n        ).sort(sqlf.asc('time')).first()\n    return event\n\n  def add_feature(self, data, equity, name):\n    \"\"\" Add another feature to stored events rdd (self.equity_events).\n    \"\"\"\n    rows = [(\n            to_epoch(index),\n            float(data.values[idx, 0]) if type(data.values[0]) == list \\\n              else float(data.values[idx])\n          ) for idx, index in enumerate(data.index)]\n    df = sqlContext.createDataFrame(rows, schema=['time', name])\n    if equity not in self.equity_events:\n      self.equity_events[equity] = df\n    else:\n      self.equity_events[equity] = \\\n        self._join_dfs(self.equity_events[equity], df)\n    \n    return self\n    \n  def add_daily_returns(self, data, equity):\n    \"\"\" Add a daily return feature.\n    \"\"\"\n    return self.add_feature(data, equity, 'daily_return')\n\n  def create_trader(self, trader_class, *args, **kwargs):\n    trader = trader_class(self, *args, **kwargs)\n    if self.Q_to_load is not None:\n      trader.load_Q(self.Q_to_load)\n    self.trader = trader\n    return trader\n  \n  def discretize(self, value, equity, feature):\n    \"\"\" Discretize a value of given equity and feature.\n\n    Usage example:\n    >>> discretize(3.3, 'GOOG', 'adj_sma_ratio')\n    \"\"\"\n    bins = self.equity_bins[equity][feature]\n    result = discretize(value, bins)\n    return result\n  \n  def reset(self, testing=False, trial_num=0):\n    \"\"\"Called at the beginning of a new trial.\n    \"\"\"\n    \n    self.done = False\n\n    # step() related variables.\n    self.current_event_ = None\n    self.previous_event_ = None\n    self.current_datetime_ = None\n    self.cash = self.capital\n    self.total_portfolio = self.capital\n\n    self.trader.reset(testing, trial_num=trial_num)\n    \n    if self.resetted is False:\n      self.resetted = True\n      for _, df in self.equity_events.iteritems():\n        df.cache()\n  \n  def step(self, event=None):\n    \"\"\" Stepping into next event.\n    \n    At the end of this step, return actions chosen by the trader.\n    \"\"\"\n    \n    # Stepping up for each equity.\n    # Todo: Is it possible to do this in parallel?\n    # Todo: If one trader needs to handle multiple equities, then\n    #       equity events might need to be structured such that\n    #       there is a single DataFrame object instead.\n    if self.current_event_ is not None:\n      self.previous_event_ = self.current_event_\n\n    if event is not None:\n      self.current_event_ = event\n    else:\n      events_df = self.equity_events[self.trader.equity]\n      self.current_event_ = self.next_event(events_df)\n      \n    if self.current_event_ is None:\n      self.done = True\n      q_cell = {}\n      varlog = {}\n    else:\n      self.current_datetime_ = self.current_event_['time']\n      (q_cell, varlog) = self.trader.update(self.current_event_)\n    self.log(\"epsilon: {}\".format(self.trader.epsilon))\n    self.log(\"set event to event ID {}\".format(self.current_datetime_))\n    return (q_cell, varlog)\n\n  def evaluate_last_action(self, trader):\n    \"\"\"Evaluate trader's previous action.\n    \"\"\"\n    varlog = {}\n    reward = None\n    next_state = None\n    if trader.last_state is not None:\n      event = self.previous_event_\n      daily_return = self.current_event_['daily_return']\n      \n      # For reward calculation\n      previous_close = event['adj_close']\n      \n      # For portfolio calculation\n      current_close = self.current_event_['adj_close']\n      \n      hold_size = trader.hold_size\n      initial_hold_size = hold_size\n      action = trader.last_action\n      action_size = 0\n      clearance_size = trader.clearance_pct/100 * self.cash\n      \n      # Daily return since entry\n      new_return = trader.return_since_entry\n      \n      commission = 0\n      \n      state = trader.last_state\n      self.log(\"allow short? {}\".format(trader.allow_short))\n      self.log(\"evaluate state {}\".format(state))\n      self.log(\"  {} hold size (before action): {}\".format(\n          state, hold_size))\n      self.log(\"  {} previous close price: {}\".format(\n          state, previous_close))\n      self.log(\"  {} current close price: {}\".format(\n          state, current_close))\n      self.log(\"  {} daily return: {}\".format(state, daily_return))\n      \n      # Calculate reward from doing the action\n      # --------------------------------------\n      # Todo: For our first iteration, the agent would just\n      #       go all-in when buying or selling. This means\n      #       when the trader is in LONG position (hold_size > 0),\n      #       it can't buy any more, and vice-versa for SHORT position.\n      if hold_size > 0:\n        if action is 'buy':\n          # Todo: Later we may allow the agent to buy more.\n          new_return += daily_return * hold_size\n          action_size = 0 \n        elif action is 'sell':\n          reward = new_return\n          hold_size = 0\n          new_return = 0\n          trader.entry_price = 0\n          self.cash += initial_hold_size * previous_close\n          if self.commission is not None:\n            commission = self.commission.calculate(\n              initial_hold_size)\n          action_size = initial_hold_size \n        elif action is 'hold':\n          new_return += daily_return * hold_size\n          action_size = initial_hold_size       \n         \n      elif hold_size == 0:\n        if action is 'buy':\n          hold_size = int(clearance_size/previous_close)\n          new_return += daily_return * hold_size\n          trader.entry_price = previous_close\n          self.cash -= hold_size * previous_close\n          if self.commission is not None:\n            commission = self.commission.calculate(hold_size)\n        elif action is 'sell':\n          hold_size = -int(clearance_size/previous_close)\n          new_return -= daily_return * hold_size\n          trader.entry_price = previous_close\n          self.cash -= hold_size * previous_close\n          if self.commission is not None:\n            commission = self.commission.calculate(hold_size)\n        elif action is 'hold':\n          pass\n        action_size = hold_size\n        \n      elif hold_size < 0:\n        if action is 'buy':\n          reward = new_return\n          hold_size = 0\n          new_return = 0\n          trader.entry_price = 0\n          self.cash += initial_hold_size * previous_close\n          if self.commission is not None:\n            commission = self.commission.calculate(initial_hold_size)\n          action_size = initial_hold_size\n        elif action is 'sell':\n          new_return -= daily_return * hold_size\n          action_size = initial_hold_size\n        elif action is 'hold':\n          new_return -= daily_return * hold_size\n          action_size = 0\n      # --------------------------------------\n      \n      if reward is None:\n        reward = new_return\n\n      self.cash -= commission\n      self.log(\"  {} action: {} {}\".format(\n          state, action, action_size))\n      self.log(\"  {} new return: {}\".format(state, new_return))\n      self.log(\"  {} reward: {}\".format(state, reward))\n      self.log(\"  {} cash: {}\".format(state, self.cash))\n      previous_total_portfolio = self.cash + \\\n        (hold_size * previous_close)\n      self.total_portfolio = self.cash + (hold_size * current_close)\n      self.log(\"  {} total portfolio: {}\".format(\n          state, previous_total_portfolio))\n      \n      next_state = trader.create_state(\n        event=event,\n        return_since_entry=new_return,\n        hold_size=hold_size,\n        update=True\n      )\n      self.log(\"  {} next state: {}\".format(state, next_state))\n      varlog['time'] = event['time']\n      varlog['state'] = state\n      varlog['cash'] = self.cash\n      varlog['total_portfolio'] = previous_total_portfolio\n      varlog['return_pct'] = (previous_total_portfolio - self.capital)/self.capital\n      varlog['close'] = previous_close\n      varlog['next_close'] = current_close\n      varlog['daily_return'] = daily_return\n      varlog['action'] = action\n      varlog['action_size'] = action_size\n      varlog['reward'] = reward\n      varlog['initial_hold_size'] = initial_hold_size\n      varlog['final_hold_size'] = hold_size\n      \n    return (reward, next_state, varlog)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["class Logger(object):\n  def __init__(self, log_dirname, trial_filename=None, erase=False, buffer_size=65500):\n    self.log_dirname = log_dirname\n    self.trial_filename = trial_filename\n    self.erase = erase\n    self.buf = \"\"\n    self.buffer_size = buffer_size\n    if self.erase:\n      dbutils.fs.rm(self.log_dirname, recurse=True)\n      dbutils.fs.mkdirs(self.log_dirname)\n      if self.trial_filename is not None:\n        dbutils.fs.rm(self.trial_filename, recurse=True)\n      self.trial_df = None\n    else:\n      if self.trial_filename is not None:\n        self.trial_df = sqlContext.read.format('parquet') \\\n                        .load(self.trial_filename)\n  \n  def write(self, text):\n    timestr = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n    self.buf += \"[{}] {}\\n\".format(timestr, text)\n    if len(self.buf) >= self.buffer_size:\n      self.save(save_trial=False)\n  \n  def save(self, save_trial=True):\n    if len(dbutils.fs.ls(self.log_dirname)) == 0:\n      filename = 'log_1.txt'\n    else:\n      last_idx = sorted(map(\n        lambda f: int(f.name.split('_')[1].split('.')[0]),\n          dbutils.fs.ls(self.log_dirname)))[-1]\n      filename = 'log_{}.txt'.format(last_idx+1)\n    \n    path = os.path.join(self.log_dirname, filename)\n    dbutils.fs.put(path, self.buf, overwrite=True)\n    self.buf = \"\"\n    if save_trial and self.trial_df is not None:\n      self.trial_df.write.format('parquet') \\\n        .save(self.trial_filename, mode='overwrite')\n  \n  def p(self, begin_with=None):\n    \"\"\" Print from file.\n\n    Sample use: `logger.p(begin_with='(Chosen|values)')` will print\n    everything that begins with \"Chosen\" or \"values\".\n    \n    Args: \n      begin_with: Read lines that begin with given regex\n    \"\"\"\n    for f in dbutils.fs.ls(self.log_dirname):\n      path = str(f.path)\n      if begin_with is not None:\n        for match in re.finditer(\n          '\\[.*\\] {}.*'.format(begin_with), dbutils.fs.head(path)):\n          print match.group(0)\n      else:\n        print dbutils.fs.head(path)\n\n  def log_trial(self, **kwargs):\n    row = {}\n    for arg in kwargs:\n      row[arg] = kwargs[arg]\n\n    schema = StructType([\n        StructField('training', BooleanType(), True),\n        StructField('trial', IntegerType(), True),\n        StructField('equity', StringType(), True),\n        StructField('start_time', TimestampType(), True),\n        StructField('end_time', TimestampType(), True),        \n        StructField('times_profit', IntegerType(), True),\n        StructField('times_draw', IntegerType(), True),\n        StructField('times_loss', IntegerType(), True),\n        StructField('total_rewards', FloatType(), True),\n        StructField('total_portfolio', FloatType(), True),\n        StructField('return_pct', FloatType(), True),\n        StructField('parameters', MapType(\n            StringType(), FloatType()), True)\n      ])\n\n    df = sqlContext.createDataFrame(\n      sc.parallelize([row]), schema=schema)\n    if self.trial_df is None:\n      self.trial_df = df\n    else:\n      self.trial_df = self.trial_df.unionAll(df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["class Trader(object):\n  def __init__(self, env, equity=None, allow_short=False):\n    self.env = env\n    self.experiences = []\n    self.equity = equity\n    self.allow_short = allow_short\n    self.hold_size = 0\n    \n    # return_since_entry = hold_size * equity value.\n    self.return_since_entry = 0\n    self.valid_actions = [\n      'hold',\n      'buy',\n      'sell'\n    ]\n    \n  def update(self, event):\n    \"\"\" Run from Environment class each time there is a new event.\n    \"\"\"\n    pass\n    \nclass RLTrader(Trader):\n  \"\"\" Reinforcement-Learning-powered Trader\n  \"\"\"\n  def __init__(self, env, equity=None, learning=True, learning_rate=0.2,\n               discount_rate=0.1, epsilon=1.0,\n               test_learning_rate=0.02, test_discount_rate=0.01,\n               test_epsilon=0, discretization_steps=20,\n               allow_short=True, clearance_pct=100,\n               earning_pct_bins=[-0.03, -0.01, 0, 0.005, 0.01, 0.02],\n               config=None, Q={}, epsilon_decay_f=None):\n    \"\"\" Initialize trader\n    \n    Args:\n      clearance_pct(float): Percentage of capital the agent\n                            is allowed to trade.\n    \"\"\"\n    super(RLTrader, self).__init__(env, equity=equity, allow_short=allow_short)\n    \n    self.learning_rate = learning_rate\n    self.discount_rate = discount_rate\n    self.epsilon = epsilon\n    self.test_learning_rate = test_learning_rate\n    self.test_discount_rate = test_discount_rate\n    self.test_epsilon = test_epsilon\n    self.Q = Q\n    self.discretization_steps=discretization_steps\n    self.learning = learning\n    self.clearance_pct = clearance_pct\n    self.earning_pct_bins = earning_pct_bins\n    self.entry_price = 0\n    self.epsilon_decay_f = epsilon_decay_f\n    if config is not None:\n      for key in config.keys():\n        setattr(self, key, config[key])\n        \n    if self.equity in self.Q:\n      self.Q = self.Q[self.equity]\n  \n  def reset(self, testing=False, trial_num=0):\n    self.last_state = None\n    self.last_action = None\n    self.last_exp = None\n    self.testing = testing\n    self.entry_price = 0\n    self.hold_size = 0\n    \n#     self.epsilon -= 0.05\n    if self.learning:\n      if self.epsilon_decay_f is not None:\n        self.epsilon = self.epsilon_decay_f(trial_num)\n      else:\n        self.epsilon = math.cos(0.05*trial_num)\n\n    \n  def update(self, event):\n    if self.last_action is not None:\n      (reward, next_state, varlog) = self.env.evaluate_last_action(self)\n      self.create_Q(next_state)\n      self.add_experience(\n        state=self.last_state,\n        action=self.last_action,\n        reward=reward,\n        next_state=next_state\n      )\n    else:\n      # First update\n      next_state = self.create_state(\n        event=event,\n        hold_size=self.hold_size,\n        return_since_entry=self.return_since_entry\n      )\n      self.create_Q(next_state)\n      varlog = {}\n    \n    self.last_action = self.choose_action(next_state)\n    self.last_state = next_state\n    return (self.Q[next_state], varlog)\n  \n  def get_maxQ(self, state, valid_actions):\n    \"\"\" The get_max_Q function is called when the trader is asked to\n    find the maximum Q-value of all actions based on the 'state'\n    the trader is in.\n    \n    \"\"\"\n\n    maxQ = None\n    for key in valid_actions:\n      if (maxQ == None) or (maxQ < self.Q[state][key]):\n        maxQ = self.Q[state][key]\n    return maxQ \n  \n  def choose_action(self, state):\n    # Set the agent state and default action\n    action = 'hold'\n    \n    if self.testing:\n      epsilon = self.test_epsilon\n    else:\n      epsilon = self.epsilon\n      \n    # If held no equity and short is not allowed, do not sell\n    if self.hold_size == 0 and self.allow_short is False:\n      valid_actions = list(self.valid_actions)\n      valid_actions.remove('sell')\n    else:\n      valid_actions = list(self.valid_actions)\n\n    # When not learning, choose a random action\n    # When learning, choose a random action with 'epsilon' probability\n    #   Otherwise, choose an action with the highest Q-value for\n    #   the current state\n    n = len(valid_actions) - 1\n    if self.learning:\n      rand = randint(0, 99)\n      if(rand < epsilon*100):\n        action  = valid_actions[randint(0, n)]\n        self.env.log(\n          \"Chosen action '{}' randomly ({} < epsilon {})\".format(\n            action, rand/100.0, epsilon))\n      else:\n        # Find the maximum Q-value for the given state\n        maxQ = self.get_maxQ(state, valid_actions)\n\n        # Choose a best action based on that value\n        action = random.choice([action for action in \\\n          valid_actions if self.Q[state][action] == maxQ])\n        self.env.log(\n          \"Chosen max action '{}'\".format(\n            action))\n    else:\n      action  = valid_actions[randint(0, n)]\n      self.env.log(\n        \"Chosen action {} randomly (not learning)\".format(\n          action))\n    return action\n  \n  def create_state(self, event=None,\n                   return_since_entry=None,\n                   hold_size=None,\n                   update=False):\n    \"\"\" Create a state based on an inputs dictionary.\n    \"\"\"\n    # Todo: return_since_entry is tricky since we don't know what\n    #       thresholds we should use in its discretization step,\n    #       so let's exclude it for now.\n    values = []\n    features = ['adj_sma_ratio',\n                'bollinger_upper',\n                'bollinger_lower',\n                'daily_return']\n    self.env.log(\"event: {}\".format(event))\n    for feature in features:\n      self.env.log(\n        \"  setting event of feature {}\".format(feature))\n      self.env.log(\n        \"  {}\".format(event[feature]))\n      value = event[feature]\n      values.append(self.env.discretize(\n          value,\n          self.equity,\n          feature\n        ))\n    \n    if hold_size*self.entry_price == 0:\n      return_pct = 0\n    else:\n      return_pct = return_since_entry/(hold_size*self.entry_price)\n      \n    self.env.log(\n      \"return_since_entry: {}\".format(return_since_entry))\n    self.env.log(\n      \"hold_size: {}\".format(hold_size))\n    self.env.log(\n      \"entry_price: {}\".format(self.entry_price))\n    self.env.log(\n      \"return_pct: {}\".format(return_pct))\n    values.append(\n      discretize(\n        return_pct,\n        self.earning_pct_bins\n      )\n    )\n    if hold_size > 0:\n      hold_position = 1\n    elif hold_size == 0:\n      hold_position = 0\n    else:\n      hold_position = -1\n    values.append(discretize(hold_position,\n                             np.array([-1,0,1])))\n    digits = len(str(self.discretization_steps))\n    state = combine_ints(values, digits=digits)\n    self.env.log(\n      \"values: {}\".format(values))\n    self.env.log(\n      \"state: {}\".format(state))\n    \n    if update is True:\n      self.return_since_entry = return_since_entry\n      self.hold_size = hold_size\n      \n    return state\n\n  def create_Q(self, state):\n    \"\"\" The create_Q function is called when a state is\n    generated by the agent. \"\"\"\n\n    # Check if the 'state' is not in the Q-table\n    # If it is not, create a new dictionary for that state.\n    # Then, for each action available, set the initial Q-value to 0.\n    if not (state in self.Q.keys()):\n      self.Q[state] = {}\n      for action in self.valid_actions:\n        self.Q[state][action] = 0\n    return\n\n  def add_experience(self, state=None, action=None, reward=None,\n                     next_state=None):\n    \"\"\" Add experience dictionary to trader's Q.\n    \"\"\"\n    if self.learning:\n      if self.testing:\n        learning_rate = self.test_learning_rate\n        discount_rate = self.test_discount_rate\n      else:\n        learning_rate = self.learning_rate\n        discount_rate = self.discount_rate\n\n      old_value = (1-learning_rate)*self.Q[state][action]\n      disc_value = discount_rate*self.Q[next_state][action]\n      new_value = learning_rate*(reward+disc_value)\n      self.Q[state][action] = int(old_value + new_value)\n      self.last_exp = {'state': state,\n                       'action': action,\n                       'reward': reward,\n                       'next_state': next_state}\n    else:\n      self.Q[state][action] = 0\n      self.last_exp = {'state': state,\n                       'action': action,\n                       'reward': reward,\n                       'next_state': next_state}"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["class Simulator(object):\n  \"\"\" This class helps us to get started without having to trade in real time.\n  \"\"\"\n  def __init__(self, env, save_config_to=None,\n               hard_limit=100, trial=1):\n    self.env = env\n    self.save_config_to = save_config_to\n    \n    self.quit = False\n    self.hard_limit = hard_limit\n    self.trial = trial\n        \n  def _save_config(self):\n    config = {\n      'simulation': {\n        'hard_limit': self.hard_limit,\n        'trial': self.trial\n      },\n      'env': {\n        'equity_bins': self.env.equity_bins,\n        'discretization_steps': self.env.discretization_steps\n      },\n      'commission': {\n        'per_share': self.env.commission.per_share,\n        'min_cost': self.env.commission.min_cost\n      },\n      'trader': {\n        'allow_short': self.env.trader.allow_short,\n        'equity': self.env.trader.equity,\n        'earning_pct_bins': self.env.trader.earning_pct_bins,\n        'clearance_pct': self.env.trader.clearance_pct,\n        'discretization_steps': self.env.trader.discretization_steps,\n        'epsilon': self.env.trader.epsilon,\n        'learning_rate': self.env.trader.learning_rate,\n        'discount_rate': self.env.trader.discount_rate\n      }\n    }\n    dbutils.fs.put(self.save_config_to, json.dumps(config),\n                   overwrite=True)\n    \n  def run(self, n_test=10, tolerance=0.005):\n    if len(self.env.equity_bins.keys()) == 0:\n      self.env.prepare_discretization_bins()\n    testing = False\n    # Todo: Multiple traders or one trader handles multiple equities?\n    trader = self.env.trader\n    trial = 1\n    self.env.log(\"training starts\")\n    self.env.reset(testing, trial_num=trial)\n    equity = self.env.trader.equity\n    start_time = to_datetime(self.env.equity_events[equity].sort('time').first()['time'])\n    end_time = to_datetime(self.env.equity_events[equity].sort(sqlf.desc('time')).first()['time'])\n    while True:\n      if not testing:\n        self.env.log(\"trader is learning, epsilon: {}, trial: {}\".format(trader.epsilon, trial))\n        if trader.epsilon < tolerance or trial > self.hard_limit:\n          self.env.log(\"testing starts\")\n          testing = True\n          trial = 1\n\n      else:\n        if trial > n_test:\n          break\n      self.env.reset(testing, trial_num=trial)\n      total_rewards = 0\n      times_profit = 0\n      times_loss = 0\n      times_draw = 0\n      while True:\n        self.env.log(\"stepping\")\n        (action, varlog) = self.env.step()\n        if trader is not None and trader.last_exp is not None:\n          total_rewards += trader.last_exp['reward']\n          if trader.last_exp['reward'] > 0:\n            times_profit += 1\n          if trader.last_exp['reward'] == 0:\n            times_draw += 1\n          if trader.last_exp['reward'] < 0:\n            times_loss += 1\n        if self.env.done:\n          break\n  \n      self.env.log_trial(\n        training=(not testing),\n        trial=trial,\n        equity=trader.equity,\n        start_time=start_time,\n        end_time=end_time,\n        times_profit=times_profit,\n        times_draw=times_draw,\n        times_loss=times_loss,\n        total_rewards=total_rewards,\n        total_portfolio=self.env.total_portfolio,\n        return_pct=(self.env.total_portfolio - self.env.capital)/self.env.capital,\n        parameters={\n          'a': trader.learning_rate if not testing else trader.test_learning_rate,\n          'e': trader.epsilon if not testing else trader.test_epsilon,\n          'd': trader.discount_rate if not testing else trader.test_discount_rate\n        }\n      )\n      trial += 1\n      \n      if self.save_config_to is not None:\n        self._save_config()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def get_rolling_mean(values, window):\n  \"\"\" Return rolling mean of given values, using specified window size.\n  \"\"\"\n  return pd.rolling_mean(values, window=window)\n\n\ndef get_rolling_std(values, window):\n  \"\"\" Return rolling standard deviation of given values, using specified window size.\n  \"\"\"\n  return pd.rolling_std(values, window=window)\n\n\ndef get_bollinger_bands(rm, rstd):\n  \"\"\" Return upper and lower Bollinger Bands. \"\"\"\n  upper_band = rm + 2 * rstd\n  lower_band = rm - 2 * rstd\n  return upper_band, lower_band"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["def max_dd(ser):\n  max2here = pd.expanding_max(ser)\n  dd2here = ser - max2here\n  return dd2here.min()\n\ndef visualize(plt, trial_df, test_df, close_df, capital=100000, learning=True):\n  fig = plt.figure(figsize=(13,17))\n  gs_train = plt.GridSpec(4, 2)\n  gs_test = plt.GridSpec(4, 2)\n  gs_details = plt.GridSpec(4, 2)\n  trial1 = fig.add_subplot(gs_train[0,0])\n  trial2 = fig.add_subplot(gs_train[0,1])\n  params = fig.add_subplot(gs_train[1,0])\n  test_params = fig.add_subplot(gs_train[1,1])\n  test_params.axis('off')\n  # Left and right axes:\n  test2 = fig.add_subplot(gs_test[2,:], frameon=False)\n  test = fig.add_subplot(gs_test[2,:], sharex=test2)\n\n#   test = fig.add_subplot(gs_test[2,:])\n  details = fig.add_subplot(gs_details[3,0])\n  details.axis('off')\n  ab = fig.add_subplot(gs_details[3,1])\n  \n  fmt = '${x:,.0f}'\n  tick_dollar = mtick.StrMethodFormatter(fmt)\n  fmt1 = '{x:,.2%}'\n  tick_percentage = mtick.StrMethodFormatter(fmt1)\n  \n  first_idx = test_df.index[0]\n  last_idx = max(test_df.index)\n  start_date_obj = test_df['time'][first_idx]\n  start_date = start_date_obj.strftime('%Y-%m-%d')\n  end_date = test_df['time'][last_idx].strftime('%Y-%m-%d')\n\n  spy = quandl.get('GOOG/NYSE_SPY', start_date=start_date,\n           end_date=end_date, transform='rdiff')['Close']\n  spy.ix[start_date_obj] = 0.0  # adding a row\n  spy = spy.sort_index()\n\n  # Training Returns\n  \n  trial_train = trial_df[trial_df['training']==True]\n  \n  trial1.plot(trial_train['trial'], trial_train['total_portfolio'].apply(lambda x: (x-capital)/capital))\n  trial1.set_title(\"Training Portfolio\\n(starting capital ${:,})\".format(capital))\n  trial1.get_yaxis().set_major_formatter(tick_percentage)\n  trial1.set_xlim((1, max(trial_train['trial'])))\n  trial1.set_xlabel(\"Trial\")\n  trial1.set_ylabel(\"Total Return\")\n  trial1.grid(True)\n  \n  # Training PDL\n  \n  p, = trial2.plot(trial_train['trial'], trial_train['times_profit'], color='green', label=\"# Profit\")\n  d, = trial2.plot(trial_train['trial'], trial_train['times_draw'], color='#AA8939', linestyle='-', label=\"# Draw\")\n  l, = trial2.plot(trial_train['trial'], trial_train['times_loss'], color='r', linestyle='-', label=\"# Loss\")\n  trial2.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n                ncol=3, fontsize=10)\n  trial2.set_xlim((1, max(trial_train['trial'])))\n  trial2.set_xlabel(\"Trial\")\n  trial2.set_ylabel(\"Number of Occurences\")\n  trial2.grid(True)\n  trial2.text(0.5, 1.16, \"Training Profits, Draws, and Losses\",\n              horizontalalignment='center',\n              transform=trial2.transAxes,\n              fontsize=14)\n  \n  # Training Parameters\n  \n  if not learning:\n    params.axis('off')\n    params.text(0.5, 0.5, \"Learning Disabled\",\n                horizontalalignment='center',\n                verticalalignment='center',\n                fontsize=20)\n  else:\n    learning_fct = params.plot(\n      trial_train['trial'], trial_train['parameters'].apply(lambda x: x['a']),\n      label=\"Learning Factor\")\n    exploration_fct = params.plot(\n      trial_train['trial'], trial_train['parameters'].apply(lambda x: x['e']),\n      'g--', label=\"Exploration Factor\")\n    discount_fct = params.plot(\n      trial_train['trial'], trial_train['parameters'].apply(lambda x: x['d']),\n      'r-.', label=\"Discount Factor\")\n    params.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n                  ncol=3, fontsize=10)\n    params.grid(True)\n    params.set_xlim((1, max(trial_train['trial'])))\n    params.set_xlabel(\"Trial\")\n    params.set_ylabel(\"Parameter Value\")\n    params.text(0.5, 1.16, \"Training Parameters\",\n                horizontalalignment='center',\n                transform=params.transAxes,\n                fontsize=14)\n  \n  # More Details\n  \n  test_params.text(0, 1.16, 'More Details', fontweight='bold')\n  test_learning_fct = trial_df.tail(1)['parameters'].values[0]['a']\n  test_exploration_fct = trial_df.tail(1)['parameters'].values[0]['e']\n  test_discount_fct = trial_df.tail(1)['parameters'].values[0]['d']\n  train_num_days = trial_train['times_profit'][0] + \\\n    trial_train['times_draw'][0] + trial_train['times_loss'][0]\n  text = \"\"\"Equity: ${}\n\nTraining:\nDate range: {} to {}\nNum. trading days: {}\nAvg. close price: ${:,.2f}\nMax. close price: ${:,.2f}\nMin. close price: ${:,.2f}\nStandard Deviation: ${:,.2f}\n\nTesting:\nDate range: {} to {}\nNum. trading days: {}\nAvg. close price: ${:,.2f}\nMax. close price: ${:,.2f}\nMin. close price: ${:,.2f}\nStandard Deviation: ${:,.2f}\n\nTesting Learning Factor: {}\nTesting Discount Factor: {}\"\"\".format(\n    trial_train['equity'][0],\n    trial_train['start_time'][0].strftime('%Y-%m-%d'),\n    trial_train['end_time'][0].strftime('%Y-%m-%d'),\n    train_num_days,\n    close_df.ix[0:train_num_days, 'Adj. Close'].mean(),\n    close_df.ix[0:train_num_days, 'Adj. Close'].max(),\n    close_df.ix[0:train_num_days, 'Adj. Close'].min(),\n    close_df.ix[0:train_num_days, 'Adj. Close'].std(),\n    \n    start_date,\n    end_date,\n    len(test_df),\n    test_df['close'].mean(),\n    test_df['close'].max(),\n    test_df['close'].min(),\n    test_df['close'].std(),\n    test_learning_fct,\n    test_discount_fct\n  )\n  test_params.text(0, 1.07, text, verticalalignment='top')\n  \n  # Test Performance  \n  yvar = 'return_pct'\n  ylabel1 = \"Daily Return\"\n  ylabel2 = \"Benchmark (SPY)\"\n  line1 = test.plot(test_df.index, test_df[yvar])\n  line2 = test.plot(test_df.index, spy, 'r--')\n  test.set_xticks(test_df.index)\n  test.set_xticklabels(test_df['time'].apply(lambda x: x.strftime('%Y-%m-%d')))\n  for tick in test.get_xticklabels():\n    tick.set_rotation(45)\n    tick.set_horizontalalignment('right')\n  test.text(0.5, 1.16, \"Testing Performance\",\n            horizontalalignment='center',\n            transform=test.transAxes,\n            fontsize=14)\n  test.set_xlim((min(test_df.index), max(test_df.index)))\n  test.get_yaxis().set_major_formatter(tick_percentage)\n  test.set_ylabel(ylabel1)\n  test.set_xlabel(\"Date\")\n  test.grid(True)\n  \n  # Right axis line\n  ylabel3 = \"Close Price\"\n  \n  line3 = test2.plot(test_df.index,\n                     test_df['close'], 'k--',\n                     color='#B8B3B3')\n  test2.yaxis.tick_right()\n  test2.yaxis.set_label_position(\"right\")\n  test2.set_ylabel(ylabel3)\n  plt.setp(test2.get_xticklabels(), visible=False)\n  test2.get_yaxis().set_major_formatter(tick_dollar)\n\n  # Actions\n  yvar = 'close'\n  point1 = test2.plot(test_df[test_df['action']=='hold'].index, test_df[test_df['action']=='hold'][yvar], 'wo', label=\"Hold\")\n  point2 = test2.plot(test_df[test_df['action']=='buy'].index, test_df[test_df['action']=='buy'][yvar], 'go', label=\"Buy\")\n  point3 = test2.plot(test_df[test_df['action']=='sell'].index, test_df[test_df['action']=='sell'][yvar], 'r^', label=\"Sell\")\n\n  action_sizes = test_df['action_size']\n  t=test2.get_xticks().tolist()\n  ymin = min(t)\n  ymax = max(t)\n  \n  for i, value in enumerate(action_sizes):\n    idx = action_sizes.index[i]\n    text = \"{} {}\".format(test_df['action'][idx], value)\n    yrange = (ymax - ymin)\n    if (test_df[yvar][idx] - ymin) < yrange*10/100:\n      rotation = 45\n      ha = 'left'\n      va = 'bottom'\n    else:\n      rotation = -45\n      ha = 'left'\n      va = 'top'\n\n    if test_df.index[idx] == max(test_df.index):\n      ha = 'right'\n      va = 'bottom'\n    test2.annotate(text, (test_df.index[idx],\n                         test_df[yvar][idx]),\n                  fontsize=9,\n                  verticalalignment=va,\n                  horizontalalignment=ha,\n                  rotation=rotation)\n    \n    \n  # Create legends manually with artist rendition:\n  # http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n  line1_a = mlines.Line2D([], [], color='blue', label=ylabel1)\n  line2_a = mlines.Line2D([], [], color='red', linestyle='--', label=ylabel2)\n  line3_a = mlines.Line2D([], [], color='#B8B3B3', linestyle='--', label=ylabel3)\n  point1_a = mlines.Line2D([], [], color='white', marker='o', label=\"Hold\")\n  point2_a = mlines.Line2D([], [], color='white', markerfacecolor='green', marker='o', label=\"Buy\")\n  point3_a = mlines.Line2D([], [], color='white', markerfacecolor='red', marker='^', label=\"Sell\")\n  test.legend(\n    handles=[line1_a, line2_a, line3_a, point1_a, point2_a, point3_a],\n    loc='upper center', bbox_to_anchor=(0.5, 1.15),\n              ncol=6, fontsize=10)\n  \n  # Details\n  tdf = get_treasury_data(start_date, end_date)\n  rfr = tdf[select_test_df_treasury_duration(test_df)]\n  spy_returns = make_tz_aware(spy) - rfr\n  m_returns = make_test_df_tz_aware(test_df)['return_pct'] - rfr\n  ab.scatter(spy_returns, m_returns)\n  (b, a) = np.polyfit(spy_returns, m_returns, 1)\n  f = np.poly1d((b, a))\n  \n  alpha = a\n  beta = b\n  # Result of `calculate_beta` should equal `b`.\n#   beta = calculate_beta(test_df, spy, tdf)\n  sharpe = calculate_sharpe(test_df, tdf)\n  spy_sharpe = calculate_sharpe(spy, tdf, col=None)\n  \n  details.text(0, 1.09, 'Testing Performance Result', fontweight='bold',\n               verticalalignment='top')\n  dtext = \"\"\"\nStarting Capital: ${:,.0f}\nFinal Portfolio: ${:,.0f}\nTotal Returns: {:.2%}\nAlpha: {:.2f}\nBeta: {:.2f}\nSharpe Ratio: {:.2f}\nSPY Sharpe Ratio: {:.2f}\nMax Drawdown: {:.2%}\"\"\".format(\n    capital,\n    test_df.tail(1)['total_portfolio'].values[0],\n    test_df.tail(1)['return_pct'].values[0],\n    alpha,\n    beta,\n    sharpe,\n    spy_sharpe,\n    max_dd(test_df['return_pct'])\n  )\n  details.text(0, 1.0, dtext,\n               verticalalignment='top')\n\n  # Alpha and Beta\n  rng1 = max(m_returns) - min(m_returns)\n  rng2 = max(spy_returns) - min(spy_returns)\n  if rng1 > rng2:\n    lim = (min(m_returns)-rng1/10, max(m_returns)+rng1/10)\n  else:\n    lim = (min(spy_returns)-rng2/10, max(spy_returns)+rng2/10)\n  ab.set_xlim(lim)\n  ab.set_ylim(lim)\n\n  x = ab.get_xticks().tolist()\n  ab.plot(x, f(x), 'r--')\n  ab.get_yaxis().set_major_formatter(tick_percentage)\n  ab.get_xaxis().set_major_formatter(tick_percentage)\n  ab.set_xlabel(\"SPY Daily Return\")\n  ab.set_ylabel(\"Model Daily Return\")\n  ab.text(1, 0.96, \"y=%.6fx+(%.6f)\"%(b,a),\n          horizontalalignment='right',\n          verticalalignment='top',\n          transform=ab.transAxes)\n  ab.set_title(\"Daily Returns\")\n  \n  plt.tight_layout(h_pad=2)\n  display(plt.show())\n  \n# visualize(plt, e2_trial_df, e2_test_df, close_df=e2_close_df, capital=100000)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["# Experiments"],"metadata":{}},{"cell_type":"code","source":["def split_df(df, end):\n  total = len(df)\n  return (df.head(total - end), df.tail(end))\n  \ndef experiment(start_date, end_date, equity,\n               recalculate=False, plt=None,\n               rolling_window=20, test_days=20,\n               learning=True, capital=100000,\n               dsteps=10, a=0.2, e=1.0, d=0.1, ta=0, te=0, td=0,\n               epsilon_decay_f=None, allow_short=False,\n               epb=[-0.03, -0.01, 0, 0.005, 0.01, 0.02],\n               hard_limit=200, tol=0.005,\n               trial_logfile=TRIAL_LOGFILE, test_logfile=TEST_LOGFILE,\n               config_file=CONFIG_FILE, Q_file=Q_FILE,\n               project_dir=PROJECT_DIR, logfile=LOGFILE):\n  # Training\n  trial_logfile = os.path.join(project_dir, trial_logfile)\n  test_logfile = os.path.join(project_dir, test_logfile)\n  \n  # If not recalculate and files already exist, just load them\n  # and visualize.\n  load_dfs = False\n  if not recalculate:\n    try:\n      dbutils.fs.ls(trial_logfile)\n      dbutils.fs.ls(test_logfile)\n      load_dfs=True\n    except:\n      print \"{} or {} not found, recalculate.\".format(\n        trial_logfile, test_logfile)\n\n  adj = quandl.get(equity, start_date=start_date,\n                   end_date=end_date, column_index=11)\n\n  if load_dfs:\n    trial_df = sqlContext.read.format('parquet') \\\n      .load(trial_logfile).toPandas()\n\n    test_df = sqlContext.read.format('parquet') \\\n      .load(test_logfile).toPandas()\n  else:\n    Q_file = os.path.join(project_dir, Q_file)\n    logfile = os.path.join(project_dir, logfile)\n    trial_logfile = os.path.join(\n      project_dir, trial_logfile)\n    config_file = os.path.join(project_dir, config_file)\n\n    # Use logger = None to print out the log instead.\n    logger = Logger(logfile, trial_logfile, erase=True)\n\n    commission_mdl = Commission(per_share=0.0075, min_cost=1)\n    env = Environment(capital=capital,\n                      logger=logger,\n                      commission=commission_mdl,\n                      discretization_steps=dsteps)\n\n\n    sma = get_rolling_mean(\n      adj['Adj. Close'], rolling_window)[rolling_window-1:]\n    adj_sma_ratio = adj[rolling_window-1:]/sma\n    rstd = get_rolling_std(\n      adj['Adj. Close'], rolling_window)[rolling_window-1:]\n    upper_band, lower_band = get_bollinger_bands(sma, rstd)\n    daily_returns = quandl.get(equity,\n                               start_date=start_date,\n                               end_date=end_date,\n                               column_index=11,\n                               transform='diff')[rolling_window-1:]\n\n    num_days_min_w = len(adj_sma_ratio)\n    if num_days_min_w <= test_days:\n      raise ValueError(\n        \"number of test days (currently {}) must be smaller than number of total days minus rolling window (currently {}). Choose smaller test days or a wider range of dates.\".format(test_days, num_days_min_w))\n\n    train_adj_sma_ratio, test_adj_sma_ratio = split_df(adj_sma_ratio, test_days)\n    train_sma, test_sma = split_df(sma, test_days)\n    train_rstd, test_rstd = split_df(rstd, test_days)\n    train_upper_band, test_upper_band = split_df(upper_band, test_days)\n    train_lower_band, test_lower_band = split_df(lower_band, test_days)\n    train_adj, test_adj = split_df(adj, test_days)\n    train_daily_returns, test_daily_returns = split_df(daily_returns, test_days)\n\n    env.add_feature(train_adj, equity=equity, name='adj_close')\n    env.add_feature(train_adj_sma_ratio, equity=equity, name='adj_sma_ratio')\n    env.add_feature(train_upper_band, equity=equity, name='bollinger_upper')\n    env.add_feature(train_lower_band, equity=equity, name='bollinger_lower')\n\n    env.add_daily_returns(train_daily_returns, equity=equity)\n\n    env.create_trader(RLTrader, equity=equity,\n                      learning=learning, learning_rate=a,\n                      epsilon=e, discount_rate=d,\n                      allow_short=allow_short, test_learning_rate=ta,\n                      test_epsilon=te, test_discount_rate=td,\n                      earning_pct_bins=epb,\n                      epsilon_decay_f=epsilon_decay_f)\n    sim = Simulator(env,\n                    save_config_to=config_file,\n                    hard_limit=hard_limit)\n    sim.run(n_test=1, tolerance=tol)\n    env.save_Q(Q_file)\n\n    # Testing\n\n    env.reset(testing=True)\n    test_logfile = os.path.join(project_dir, test_logfile)\n    schema = StructType([\n        StructField('time', TimestampType(), True),\n        StructField('total_portfolio', FloatType(), True),\n        StructField('return_pct', FloatType(), True),\n        StructField('action', StringType(), True),\n        StructField('action_size', IntegerType(), True),\n        StructField('close', FloatType(), True),\n        StructField('action_total', FloatType(), True),\n        StructField('cash', FloatType(), True),\n        StructField('commission', FloatType(), True)\n      ])\n    test_df = None\n    for idx in test_adj_sma_ratio.index:\n      event = {\n               'time': to_epoch(idx),\n               'adj_close': test_adj.ix[idx, 0],\n               'adj_sma_ratio': test_adj_sma_ratio.ix[idx, 0],\n               'bollinger_upper': test_upper_band.ix[idx, 0],\n               'bollinger_lower': test_lower_band.ix[idx, 0],\n               'daily_return': test_daily_returns.ix[idx, 0]\n              }\n      (q_cell, varlog) = env.step(event)\n      if varlog is not None and 'total_portfolio' in varlog:\n        row = {\n          'time': idx,\n          'total_portfolio': float(varlog['total_portfolio']),\n          'return_pct': float(varlog['return_pct']),\n          'action': varlog['action'],\n          'action_size': varlog['action_size'],\n          'close': float(varlog['close']),\n          'action_total': float(varlog['action_size'] * varlog['close']),\n          'cash': float(varlog['cash']),\n          'commission': float(varlog['cash'])\n        }\n        df = sqlContext.createDataFrame(\n          sc.parallelize([row]), schema=schema)\n        if test_df is None:\n          test_df = df\n        else:\n          test_df = test_df.unionAll(df)\n\n    if test_df is not None:\n      test_df.write.format('parquet') \\\n        .save(test_logfile, mode='overwrite')\n\n    logger.save()\n\n    trial_df = sqlContext.read.format('parquet') \\\n      .load(trial_logfile).toPandas()\n      \n    test_df = test_df.toPandas()\n  \n  if plt is not None:\n    visualize(plt, trial_df, test_df, close_df=adj, capital=capital, learning=learning)\n  return (trial_df, test_df, adj)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Initial benchmark\n# learning rate, exploration rate, discount factor\n\nimport tabletext\n\ndef absmin(l):\n  min_v = np.inf\n  for i, v in enumerate(l):\n    if abs(v) < abs(min_v):\n      min_v = v\n  return min_v\n\ndef call(report, f, metric='sharpe'):\n  items = []\n  for a, r1 in report.iteritems():\n    row = [a]\n    for e, r2 in r1.iteritems():\n      gs = []\n      for g, r3 in r2.iteritems():\n        items.append(r3[metric])\n  f = eval(f)\n  return(f(items))\n  \ndef show(report, metric='sharpe', best='max'):\n  disp = []\n  head = ['']\n  best_v = \"{:.2f}\".format(call(report, best, metric=metric))\n  for e, r in report[report.keys()[0]].iteritems():\n    head.append(e)\n  disp.append(head)\n  for a, r1 in iter(sorted(report.iteritems())):\n    row = [a]\n    for e, r2 in iter(sorted(r1.iteritems())):\n      gs = []\n      for g, r3 in iter(sorted(r2.iteritems())):\n        v = \"{:.2f}\".format(r3[metric])\n        if v == best_v:\n          v = \"{}*\".format(v) \n        gs.append(v)\n      cell = \"{}/{}/{}\".format(*gs)\n      row.append(cell)\n    disp.append(row)\n  print tabletext.to_text(disp)\n\nresults = {}\nstart_date = '2016-05-02'\nend_date = '2016-09-14'\nequity = 'WIKI/TSLA'\nalphas = [0.1, 0.2, 0.3]\ndef f1(t):\n  return math.cos(0.05*t)\ndef f2(t):\n  return 0.05**t\ndef f3(t):\n  t2 = t**2\n  if t2 == 0:\n    return 0\n  else:\n    return 1/(t2)\nepsilons = [f1, f2, f3]\ngammas = [0.05, 0.1, 0.2]\n\nsave_to = os.path.join(PROJECT_DIR, EXP_RESULTS)\n\ntry:\n  results = json.loads(dbutils.fs.head(save_to))\nexcept:\n\n  for a in alphas:\n    if a not in results:\n      results[a] = {}\n    for e in epsilons:\n      if e.__name__ not in results[a]:\n        results[a][e.__name__] = {}\n      for g in gammas:\n        if g not in results[a][e.__name__]:\n          results[a][e.__name__][g] = {}\n        fname = 'a{}_e{}_g{}'.format(a,e.__name__,g)\n        trial_df, test_df, close_df = experiment(\n          start_date, end_date, equity, learning=True,\n          a=a, epsilon_decay_f=e, d=g,\n          trial_logfile='{}_trial.parquet'.format(fname),\n          test_logfile='{}_test.parquet'.format(fname),\n          Q_file='{}_Q.csv'.format(fname),\n          config_file='{}_config.json'.format(fname),\n          logfile='{}_log.parquet'.format(fname)\n        )\n        \n        first_idx = test_df.index[0]\n        last_idx = max(test_df.index)\n        t_start_date_obj = test_df['time'][first_idx]\n        t_start_date = t_start_date_obj.strftime('%Y-%m-%d')\n        t_end_date = test_df['time'][last_idx].strftime('%Y-%m-%d')\n        \n        tdf = get_treasury_data(t_start_date, t_end_date)\n        sharpe = calculate_sharpe(test_df, tdf)\n\n        spy = quandl.get('GOOG/NYSE_SPY', start_date=t_start_date,\n           end_date=t_end_date, transform='rdiff')['Close']\n        spy.ix[t_start_date_obj] = 0.0  # adding a row\n        spy = spy.sort_index()\n\n        rfr = tdf[select_test_df_treasury_duration(test_df)]\n        spy_returns = make_tz_aware(spy) - rfr\n        m_returns = make_test_df_tz_aware(test_df)['return_pct'] - rfr\n        (beta, alpha) = np.polyfit(spy_returns, m_returns, 1)\n        results[a][e.__name__][g] = {\n          'alpha': alpha,\n          'beta': beta,\n          'sharpe': sharpe\n        }\n\n  dbutils.fs.put(save_to, json.dumps(results),\n           overwrite=True)\nprint \"Sharpe\"\nshow(results, metric='sharpe', best='max')\nprint \"Alpha\"\nshow(results, metric='alpha', best='max')\nprint \"Beta\"\nshow(results, metric='beta', best='absmin')"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Scenario 1\ne1_trial_df, e1_test_df, e1_close_df = experiment(\n  '2014-05-09', '2015-03-27', 'WIKI/TSLA', plt=plt, learning=False,\n  allow_short=False, hard_limit=30, recalculate=False,\n  trial_logfile='e1_trial.parquet', test_logfile='e1_test.parquet',\n  Q_file='e1_Q.csv', config_file='e1_config.json', logfile='e1_log')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Scenario 2\ne2_trial_df, e2_test_df, e2_close_df = experiment(\n  '2014-05-09', '2015-03-27', 'WIKI/TSLA', plt=plt,\n  a=0.2, e=1.0, d=0.1, ta=0, te=0, td=0, allow_short=False,\n  recalculate=False,\n  trial_logfile='e2_trial.parquet',\n  test_logfile='e2_test.parquet',\n  Q_file='e2_Q.csv', config_file='e2_config.json', \n  logfile='e2_log')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Scenario 3\ne3_trial_df, e3_test_df, e3_close_df = experiment(\n  '2014-05-09', '2015-03-27', 'WIKI/TSLA', plt=plt,\n  a=0.2, e=1.0, d=0.1, ta=0.1, te=0, td=0.05, allow_short=False,\n  recalculate=False,\n  trial_logfile='e3_trial.parquet',\n  test_logfile='e3_test.parquet',\n  Q_file='e3_Q.csv', config_file='e3_config.json', \n  logfile='e3_log')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Scenario 4 - 3 months training data, agent stops learning.\ne4_trial_df, e4_test_df, e4_close_df = experiment(\n  '2014-10-27', '2015-03-27', 'WIKI/TSLA', plt=plt, learning=True,\n  allow_short=False, recalculate=False,\n  trial_logfile='e4_trial.parquet', test_logfile='e4_test.parquet',\n  Q_file='e4_Q.csv', config_file='e4_config.json', logfile='e4_log')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Scenario 5 - 3 months training data, agent keeps learning.\ne5_trial_df, e5_test_df, e5_close_df = experiment(\n  '2014-10-27', '2015-03-27', 'WIKI/TSLA', plt=plt, learning=True,\n  recalculate=False,\n  a=0.2, e=1.0, d=0.1, ta=0.1, te=0, td=0.05, allow_short=False,\n  trial_logfile='e5_trial.parquet', test_logfile='e5_test.parquet',\n  Q_file='e5_Q.csv', config_file='e5_config.json', logfile='e5_log')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["# Exploratory Visualization"],"metadata":{}},{"cell_type":"code","source":["train_df = e3_close_df['Adj. Close'][\n  str2t('2014-05-09'):str2t('2015-02-27')]\n\ntest_df = e3_close_df['Adj. Close'][\n  str2t('2015-03-03'):str2t('2015-03-27')]\n\nall_df = e3_close_df['Adj. Close'][\n  str2t('2014-05-09'):str2t('2015-03-27')]\n\nfmt = '${x:,.0f}'\ntick_dollar = mtick.StrMethodFormatter(fmt)\n\nfmt1 = '{x:,.2%}'\ntick_percentage = mtick.StrMethodFormatter(fmt1)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["ax = e3_close_df.plot()\nax.set_title(\"Adjusted Close for TSLA\")\nax.set_ylabel(\"Adjusted Close\")\nax.get_yaxis().set_major_formatter(tick_dollar)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["fig = plt.figure()\ngs_main = plt.GridSpec(1,2)\nax = fig.add_subplot(gs_main[0,0])\nax1 = fig.add_subplot(gs_main[0,1])\nax.hist(train_df)\nax.set_title(\"Training Data Distribution (TSLA)\")\nax.set_ylabel(\"Count\")\nax.set_xlabel(\"Adjusted Close\")\nax.get_xaxis().set_major_formatter(tick_dollar)\n\nax1.hist(test_df)\nax1.set_title(\"Testing Data Distribution (TSLA)\")\nax1.set_ylabel(\"Count\")\nax1.set_xlabel(\"Adjusted Close\")\nax1.get_xaxis().set_major_formatter(tick_dollar)\nax1.set_yticks([1, 2, 3, 4, 5])\n\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["dr = quandl.get('WIKI/TSLA',\n                start_date='2014-05-09',\n                end_date='2015-03-27',\n                column_index=11,\n                transform='diff')\nspy_dr = quandl.get('GOOG/NYSE_SPY',\n                    start_date='2014-05-09',\n                    end_date='2015-03-27',\n                    column_index=4,\n                    transform='diff')\n\nfig = plt.figure()\ngs_main = plt.GridSpec(1,1)\nax = fig.add_subplot(gs_main[0,0])\nax.plot(dr.index, dr['Adj. Close'], 'b-', label='TSLA')\nax.plot(spy_dr.index, spy_dr['Close'], 'r-', label='SPY')\nax.set_title(\"Daily Returns (TSLA vs SPY)\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Daily Return\")\nax.get_yaxis().set_major_formatter(tick_dollar)\nfor tick in ax.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.98),\n              ncol=2, fontsize=10)\nplt.tight_layout(h_pad=2)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["rolling_window = 20\ndr1 = get_rolling_mean(dr['Adj. Close'], rolling_window)[rolling_window-1:]\nspy_dr1 = get_rolling_mean(spy_dr['Close'], rolling_window)[rolling_window-1:]\n\nfig = plt.figure()\ngs_main = plt.GridSpec(1,1)\nax = fig.add_subplot(gs_main[0,0])\nax.plot(dr1.index, dr1, 'b-', label='TSLA')\nax.plot(spy_dr1.index, spy_dr1, 'r-', label='SPY')\nax.set_title(\"20-days Rolling Daily Returns (TSLA vs SPY)\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Daily Return\")\nax.get_yaxis().set_major_formatter(tick_dollar)\nfor tick in ax.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.98),\n              ncol=2, fontsize=10)\nplt.tight_layout(h_pad=2)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["rdr = quandl.get('WIKI/TSLA',\n                start_date='2014-05-09',\n                end_date='2015-03-27',\n                column_index=11,\n                transform='rdiff')\nrspy_dr = quandl.get('GOOG/NYSE_SPY',\n                    start_date='2014-05-09',\n                    end_date='2015-03-27',\n                    column_index=4,\n                    transform='rdiff')\nrolling_window = 20\nrdr1 = get_rolling_mean(rdr['Adj. Close'], rolling_window)[rolling_window-1:]\nrspy_dr1 = get_rolling_mean(rspy_dr['Close'], rolling_window)[rolling_window-1:]\n\n\nfig = plt.figure()\ngs_main = plt.GridSpec(1,1)\nab = fig.add_subplot(gs_main[0,0])\nab.scatter(rspy_dr1, rdr1)\n(b, a) = np.polyfit(rspy_dr1, rdr1, 1)\nf = np.poly1d((b, a))\n\nx = ab.get_xticks().tolist()\nab.plot(x, f(x), 'r--')\nab.get_yaxis().set_major_formatter(tick_percentage)\nab.get_xaxis().set_major_formatter(tick_percentage)\nab.set_xlabel(\"SPY Daily Return\")\nab.set_ylabel(\"TSLA Daily Return\")\nab.text(1, 0.96, \"y=%.6fx+(%.6f)\"%(b,a),\n        horizontalalignment='right',\n        verticalalignment='top',\n        transform=ab.transAxes)\nrng = max(rdr1) - min(rdr1)\nlim = (min(rdr1)-rng/10, max(rdr1)+rng/10)\nab.set_xlim(lim)\nab.set_ylim(lim)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["rolling_window = 20\n\nfig = plt.figure(figsize=(10, 6))\ngs_main = plt.GridSpec(2,2)\nax = fig.add_subplot(gs_main[0,0])\nax1 = fig.add_subplot(gs_main[0,1])\nax2 = fig.add_subplot(gs_main[1,0])\nax3 = fig.add_subplot(gs_main[1,1])\nax.plot(all_df.index, all_df)\nax.set_title(\"Adjusted Close of Training Data (TSLA)\")\nax.set_ylabel(\"Adjusted Close\")\nax.set_xlabel(\"Date\")\nax.get_yaxis().set_major_formatter(tick_dollar)\nfor tick in ax.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\n\nsma = get_rolling_mean(\n      all_df, rolling_window)[rolling_window-1:]\nax1.plot(sma.index, sma)\nax1.set_title(\"SMA 20-day rolling window (TSLA)\")\nax1.set_ylabel(\"SMA\")\nax1.set_xlabel(\"Date\")\nax1.get_yaxis().set_major_formatter(tick_dollar)\nfor tick in ax1.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\n\nratio = all_df[rolling_window-1:]/sma\nax2.plot(ratio.index, ratio)\nax2.set_title(\"Close SMA Ratio (TSLA)\")\nax2.set_ylabel(\"Close SMA Ratio\")\nax2.set_xlabel(\"Date\")\nfor tick in ax2.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\n\nrdr = quandl.get('WIKI/TSLA',\n                start_date=min(ratio.index).strftime('%Y-%m-%d'),\n                end_date=max(ratio.index).strftime('%Y-%m-%d'),\n                column_index=11)\nnratio = ratio / ratio.ix[0,:]\nnrdr = rdr / rdr.ix[0,:]\nadj = all_df / all_df.ix[0,:]\nax3.plot(nratio.index, nratio, label='Ratio')\nax3.plot(nrdr.index, nrdr['Adj. Close'], label='Daily Return')\nax3.plot(adj.index, adj, label='Close')\nax3.set_xlabel(\"Date\")\nfor tick in ax3.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\nax3.legend(loc='upper center', bbox_to_anchor=(0.5, 1.14),\n              ncol=3, fontsize=10)\nax3.text(0.5, 1.16, \"Normalized Ratio vs Daily Return vs Close\",\n            horizontalalignment='center',\n            transform=ax3.transAxes,\n            fontsize=14)\n  \nplt.tight_layout()\n\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["adj"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["rdr = quandl.get('WIKI/TSLA',\n                start_date=min(ratio.index).strftime('%Y-%m-%d'),\n                end_date=max(ratio.index).strftime('%Y-%m-%d'),\n                column_index=11)\nrdr / rdr.ix[0, :]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["rstd = get_rolling_std(\n  all_df, rolling_window)[rolling_window-1:]\nupper_band, lower_band = get_bollinger_bands(sma, rstd)\n\nfig = plt.figure(figsize=(8, 5))\ngs_main = plt.GridSpec(1,1)\nax = fig.add_subplot(gs_main[0,0])\nax.plot(upper_band.index, upper_band, label='Bol. upper')\nax.plot(lower_band.index, lower_band, label='Bol. lower')\nax.plot(lower_band.index, sma, label='SMA')\nax.plot(lower_band.index, all_df[rolling_window-1:], color='#B8B3B3', label='Adj. Close')\nax.set_title(\"Bollinger Bands (TSLA)\")\nax.set_ylabel(\"Adjusted Close\")\nax.set_xlabel(\"Date\")\nax.get_yaxis().set_major_formatter(tick_dollar)\nfor tick in ax.get_xticklabels():\n  tick.set_rotation(45)\n  tick.set_horizontalalignment('right')\nax.legend()\n\nplt.tight_layout()\n\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["# Archives\n\nThe blocks below are for references and testing only and do not need to be run."],"metadata":{}},{"cell_type":"code","source":["# Training\nequity = 'WIKI/TSLA'\nQ_file = os.path.join(PROJECT_DIR, Q_FILE)\nlogfile = os.path.join(PROJECT_DIR, LOGFILE)\n# trial_logfile = os.path.join(PROJECT_DIR, TRIAL_LOGFILE)\ntrial_logfile = os.path.join(\n  PROJECT_DIR, OPTIMIZED_TRIAL_LOGFILE)\nconfig_file = os.path.join(PROJECT_DIR, CONFIG_FILE)\n\n# Use logger = None to print out the log instead.\nlogger = Logger(logfile, trial_logfile, erase=True)\ncommission_mdl = Commission()\nenv = Environment(capital=100000,\n                  logger=logger,\n                  commission=commission_mdl,\n                  discretization_steps=10)\n\nstart_date = '2016-6-01'\nend_date = '2016-7-20'\nwindow = 20\n\nadj = quandl.get(equity, start_date=start_date,\n                 end_date=end_date, column_index=11)\nsma = get_rolling_mean(adj['Adj. Close'], window)[window-1:]\nadj_sma_ratio = adj[window-1:]/sma\nrstd = get_rolling_std(adj['Adj. Close'], window)[window-1:]\nupper_band, lower_band = get_bollinger_bands(sma, rstd)\n\nenv.add_feature(adj, equity=equity, name='adj_close')\nenv.add_feature(adj_sma_ratio, equity=equity, name='adj_sma_ratio')\nenv.add_feature(upper_band, equity=equity, name='bollinger_upper')\nenv.add_feature(lower_band, equity=equity, name='bollinger_lower')\n# env.add_feature(pe_ratio, equity=equity, name='pe_ratio')\n\ndaily_returns = quandl.get(equity,\n                           start_date=start_date,\n                           end_date=end_date,\n                           column_index=11,\n                           transform='diff')[window-1:]\n\nenv.add_daily_returns(daily_returns, equity=equity)\n\nenv.create_trader(RLTrader, equity=equity,\n                  learning=False, learning_rate=0.2, allow_short=True,\n                  earning_pct_bins=[-0.03, -0.01, 0, 0.005, 0.01, 0.02])\nsim = Simulator(env,\n                save_config_to=config_file,\n                hard_limit=1)\nsim.run(n_test=1, tolerance=0.005)\nlogger.save()\nenv.save_Q(Q_file)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Use this block to find the right test_start_date\ntest_start_date = '2016-11-03'\ntest_end_date = '2016-12-31'\nwindow = 20\n\nadj = quandl.get('WIKI/GOOG', start_date=test_start_date,\n                 end_date=test_end_date, column_index=11)\nsma = get_rolling_mean(adj['Adj. Close'], window)[window-1:]\nadj_sma_ratio = adj[window-1:]/sma\nadj_sma_ratio"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Testing - Keep this as a reference for real trading.\n\nequity = 'WIKI/TSLA'\nQ_file = os.path.join(PROJECT_DIR, Q_FILE)\nlogfile = os.path.join(PROJECT_DIR, LOGFILE)\n# trial_logfile = os.path.join(PROJECT_DIR, TRIAL_LOGFILE)\ntest_logfile = os.path.join(\n  PROJECT_DIR, TEST_LOGFILE)\nconfig_file = os.path.join(PROJECT_DIR, CONFIG_FILE)\n\n# Use logger = None to print out the log instead.\nlogger = Logger(logfile, test_logfile, erase=True)\nconfig = json.loads(dbutils.fs.head(config_file))\nQ = create_Q_from_file(Q_file)\ncommission_mdl = Commission(\n  per_share=config['commission']['per_share'],\n  min_cost=config['commission']['min_cost']\n)\nenv = Environment(capital=100000,\n                  logger=logger,\n                  commission=commission_mdl,\n                  config=config['env'])\nenv.create_trader(RLTrader, config=config['trader'], Q=Q)\n\n# Find a start date so that sma.indexes starts one day after\n# the training's last date\ntest_start_date = '2016-11-03'\ntest_end_date = '2016-12-31'\n\nwindow = 20\n\nadj = quandl.get(equity, start_date=test_start_date,\n                 end_date=test_end_date, column_index=11)\nsma = get_rolling_mean(adj['Adj. Close'], window)[window-1:]\nadj_sma_ratio = adj[window-1:]/sma\nrstd = get_rolling_std(adj['Adj. Close'], window)[window-1:]\nupper_band, lower_band = get_bollinger_bands(sma, rstd)\n\ndaily_returns = quandl.get(equity,\n                           start_date=test_start_date,\n                           end_date=test_end_date,\n                           column_index=11,\n                           transform='diff')[window-2:]\n\n# Begin testing\nenv.reset(testing=True)\ntest_logfile = os.path.join(PROJECT_DIR, TEST_LOGFILE)\nschema = StructType([\n    StructField('time', TimestampType(), True),\n    StructField('total_portfolio', FloatType(), True),\n    StructField('return_pct', FloatType(), True),\n    StructField('action', StringType(), True),\n    StructField('action_size', IntegerType(), True),\n    StructField('close', FloatType(), True),\n    StructField('action_total', FloatType(), True),\n    StructField('cash', FloatType(), True),\n    StructField('commission', FloatType(), True)\n  ])\ntest_df = None\nfor idx in adj_sma_ratio.index:\n  event = {\n           'time': to_epoch(idx),\n           'adj_close': adj.ix[idx, 0],\n           'adj_sma_ratio': adj_sma_ratio.ix[idx, 0],\n           'bollinger_upper': upper_band.ix[idx, 0],\n           'bollinger_lower': lower_band.ix[idx, 0],\n           'daily_return': daily_returns.ix[idx, 0]\n          }\n  (q_cell, varlog) = env.step(event)\n  if varlog is not None and 'total_portfolio' in varlog:\n    row = {\n      'time': idx,\n      'total_portfolio': float(varlog['total_portfolio']),\n      'return_pct': float(varlog['return_pct']),\n      'action': varlog['action'],\n      'action_size': varlog['action_size'],\n      'close': float(varlog['close']),\n      'action_total': float(varlog['action_size'] * varlog['close']),\n      'cash': float(varlog['cash']),\n      'commission': float(varlog['commission'])\n    }\n    df = sqlContext.createDataFrame(\n      sc.parallelize([row]), schema=schema)\n    df.set_index('time')\n    if test_df is None:\n      test_df = df\n    else:\n      test_df = test_df.unionAll(df)\n\ntest_df.write.format('parquet') \\\n  .save(test_logfile, mode='overwrite')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["trial_logfile = os.path.join(PROJECT_DIR, OPTIMIZED_TRIAL_LOGFILE)\ntrial_df = sqlContext.read.format('parquet') \\\n  .load(trial_logfile).toPandas()\n\ntest_logfile = os.path.join(PROJECT_DIR, TEST_LOGFILE)\ntest_df = sqlContext.read.format('parquet') \\\n  .load(test_logfile).toPandas()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Example on how to use logfile with regex selection\n\nlogfile = os.path.join(PROJECT_DIR, 'e5_log')\nlogger = Logger(logfile, erase=False)\nlogger.p(begin_with='({})'.format(\n  'evaluate state |  [0-9]* hold|  [0-9]* daily|  [0-9]* previous close|  [0-9]* current close|  [0-9]* action|  [0-9]* new return|  [0-9]* reward|  [0-9]* total portfolio|  [0-9]* cash'\n#   'evaluate state 90109080201|  90109080201 hold|  90109080201 daily|  90109080201 action|  90109080201 new return|  90109080201 reward|Chosen[ a-z]*action'\n  ))"],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"RL v.6 pub","notebookId":1142478279739161},"nbformat":4,"nbformat_minor":0}
